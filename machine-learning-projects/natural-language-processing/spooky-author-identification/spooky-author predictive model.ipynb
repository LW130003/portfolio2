{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author corpus\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   text\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   text\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   text\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   text\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   text"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#load dataset\n",
    "train = pd.read_csv(r\"C:\\Users\\LW130003\\Documents\\GitHub\\spooky-author-identification/train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\LW130003\\Documents\\GitHub\\spooky-author-identification/test.csv\")\n",
    "train['corpus'] = 'text'\n",
    "test['corpus'] = 'text'\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 19579\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows: {}'.format(train.shape[0]))\n",
    "print('Number of columns: {}'.format(train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "I have created two codes to create train and test corpus. First, we will measure the time they need before extend it to whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1437585353851318\n",
      "        id                                               text author  \\\n",
      "0  id26305  This process, however, afforded me no means of...    EAP   \n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
      "5  id22965  A youth passed in solitude, my best years spen...    MWS   \n",
      "6  id09674  The astronomer, perhaps, at this point, took r...    EAP   \n",
      "7  id13515        The surcingle hung in ribands from my body.    EAP   \n",
      "8  id19322  I knew that you could not say to yourself 'ste...    EAP   \n",
      "9  id00912  I confess that neither the structure of langua...    MWS   \n",
      "\n",
      "                                              corpus  \n",
      "0  process however afford mean ascertain dimensio...  \n",
      "1              never occur fumble might mere mistake  \n",
      "2  left hand gold snuff box caper hill cut manner...  \n",
      "3  lovely spring look Windsor Terrace sixteen fer...  \n",
      "4  Finding nothing else even gold Superintendent ...  \n",
      "5  youth pass solitude best year spent gentle fem...  \n",
      "6  astronomer perhaps point took refuge suggestio...  \n",
      "7                         surcingle hung riband body  \n",
      "8  knew could say stereotomy without bring think ...  \n",
      "9  confess neither structure language code govern...  \n"
     ]
    }
   ],
   "source": [
    "#method1\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "#train corpus\n",
    "#for i in np.arange(train.shape[0]):\n",
    "for i in np.arange(10):\n",
    "    text = train.text.values[i] #extract text from data set train\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) #remove punctuation\n",
    "\n",
    "    #tokenize\n",
    "    text = nltk.word_tokenize(text) #tokenize the text\n",
    "\n",
    "    #stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word.lower() not in stopwords] #remove word in stopwords\n",
    "\n",
    "    #stemming & lemmatization\n",
    "    newtext = []\n",
    "    for j, word in enumerate(text):\n",
    "        lemmword = pd.DataFrame(index=['NOUN','VERB','ADJ','ADV'], columns=['Word','Len'])    \n",
    "        lemmword.loc['NOUN'] = pd.Series({'Word':lemm.lemmatize(word, wordnet.NOUN),\n",
    "                                        'Len':len(lemm.lemmatize(word, wordnet.NOUN))})\n",
    "        lemmword.loc['VERB'] = pd.Series({'Word':lemm.lemmatize(word, wordnet.VERB),\n",
    "                                        'Len':len(lemm.lemmatize(word, wordnet.VERB))})\n",
    "        lemmword.loc['ADJ'] = pd.Series({'Word':lemm.lemmatize(word, wordnet.ADJ),\n",
    "                                        'Len':len(lemm.lemmatize(word, wordnet.ADJ))})\n",
    "        lemmword.loc['ADV'] = pd.Series({'Word':lemm.lemmatize(word, wordnet.ADV),\n",
    "                                        'Len':len(lemm.lemmatize(word, wordnet.ADV))})\n",
    "        a = lemmword.loc[lemmword['Len']==lemmword['Len'].min()]['Word'][0]\n",
    "        newtext.append(a)\n",
    "    train['corpus'].iat[i] = ' '.join(newtext)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total)\n",
    "print(train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02601790428161621\n",
      "        id                                               text author  \\\n",
      "0  id26305  This process, however, afforded me no means of...    EAP   \n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
      "5  id22965  A youth passed in solitude, my best years spen...    MWS   \n",
      "6  id09674  The astronomer, perhaps, at this point, took r...    EAP   \n",
      "7  id13515        The surcingle hung in ribands from my body.    EAP   \n",
      "8  id19322  I knew that you could not say to yourself 'ste...    EAP   \n",
      "9  id00912  I confess that neither the structure of langua...    MWS   \n",
      "\n",
      "                                              corpus  \n",
      "0  process however afford mean ascertain dimensio...  \n",
      "1              never occur fumble might mere mistake  \n",
      "2  left hand gold snuff box caper hill cut manner...  \n",
      "3  lovely spring look Windsor Terrace sixteen fer...  \n",
      "4  Finding nothing else even gold Superintendent ...  \n",
      "5  youth pass solitude best year spent gentle fem...  \n",
      "6  astronomer perhaps point took refuge suggestio...  \n",
      "7                         surcingle hung riband body  \n",
      "8  knew could say stereotomy without bring think ...  \n",
      "9  confess neither structure language code govern...  \n"
     ]
    }
   ],
   "source": [
    "#method2\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "#train corpus\n",
    "#for i in np.arange(train.shape[0]):\n",
    "for i in np.arange(10):\n",
    "    text = train.text.values[i] #extract text from data set train\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) #remove punctuation\n",
    "\n",
    "    #tokenize\n",
    "    text = nltk.word_tokenize(text) #tokenize the text\n",
    "\n",
    "    #stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word.lower() not in stopwords] #remove word in stopwords\n",
    "\n",
    "    #stemming & lemmatization\n",
    "    newtext = []               \n",
    "    for j, word in enumerate(text):\n",
    "        noun = lemm.lemmatize(word, wordnet.NOUN)\n",
    "        verb = lemm.lemmatize(word, wordnet.VERB)\n",
    "        adj = lemm.lemmatize(word, wordnet.ADJ)\n",
    "        adv = lemm.lemmatize(word, wordnet.ADV)\n",
    "        len_noun = len(noun)\n",
    "        len_verb = len(verb)\n",
    "        len_adj = len(adj)\n",
    "        len_adv = len(adv)\n",
    "        testing = True\n",
    "    \n",
    "        #len_noun is minimum\n",
    "        if testing:\n",
    "            if len_noun <= len_verb:\n",
    "                if len_noun <= len_adj:\n",
    "                    if len_noun <= len_adv:\n",
    "                        newtext.append(noun)\n",
    "                        testing = False\n",
    "\n",
    "        #len_verb is minimum\n",
    "        if testing:\n",
    "            if len_verb <= len_noun:\n",
    "                if len_verb <= len_adj:\n",
    "                    if len_verb <= len_adv:\n",
    "                        newtext.append(verb)\n",
    "                        testing = False\n",
    "\n",
    "        #len_adj is minimum\n",
    "        if testing:\n",
    "            if len_adj <= len_noun:\n",
    "                if len_adj <= len_verb:\n",
    "                    if len_adj <= len_adv:\n",
    "                        newtext.append(adj)\n",
    "                        testing = False\n",
    "\n",
    "        #len_adv is minimum\n",
    "        if testing:\n",
    "            if len_adv <= len_noun:\n",
    "                if len_adv <= len_verb:\n",
    "                    if len_adv <= len_adj:\n",
    "                        newtext.append(adv)\n",
    "                        testing = False\n",
    "    train['corpus'].iat[i] = ' '.join(newtext)\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total)\n",
    "print(train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In conclusion, two method have the same results with method 2 is around 57x faster. Next, extend method 2 to whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.920990228652954\n"
     ]
    }
   ],
   "source": [
    "#method2\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "#train corpus\n",
    "for i in np.arange(train.shape[0]):\n",
    "#for i in np.arange(10):\n",
    "    text = train.text.values[i] #extract text from data set train\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) #remove punctuation\n",
    "\n",
    "    #tokenize\n",
    "    text = nltk.word_tokenize(text) #tokenize the text\n",
    "\n",
    "    #stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word.lower() not in stopwords] #remove word in stopwords\n",
    "\n",
    "    #stemming & lemmatization\n",
    "    newtext = []               \n",
    "    for j, word in enumerate(text):\n",
    "        noun = lemm.lemmatize(word, wordnet.NOUN)\n",
    "        verb = lemm.lemmatize(word, wordnet.VERB)\n",
    "        adj = lemm.lemmatize(word, wordnet.ADJ)\n",
    "        adv = lemm.lemmatize(word, wordnet.ADV)\n",
    "        len_noun = len(noun)\n",
    "        len_verb = len(verb)\n",
    "        len_adj = len(adj)\n",
    "        len_adv = len(adv)\n",
    "        testing = True\n",
    "    \n",
    "        #len_noun is minimum\n",
    "        if testing:\n",
    "            if len_noun <= len_verb:\n",
    "                if len_noun <= len_adj:\n",
    "                    if len_noun <= len_adv:\n",
    "                        newtext.append(noun)\n",
    "                        testing = False\n",
    "\n",
    "        #len_verb is minimum\n",
    "        if testing:\n",
    "            if len_verb <= len_noun:\n",
    "                if len_verb <= len_adj:\n",
    "                    if len_verb <= len_adv:\n",
    "                        newtext.append(verb)\n",
    "                        testing = False\n",
    "\n",
    "        #len_adj is minimum\n",
    "        if testing:\n",
    "            if len_adj <= len_noun:\n",
    "                if len_adj <= len_verb:\n",
    "                    if len_adj <= len_adv:\n",
    "                        newtext.append(adj)\n",
    "                        testing = False\n",
    "\n",
    "        #len_adv is minimum\n",
    "        if testing:\n",
    "            if len_adv <= len_noun:\n",
    "                if len_adv <= len_verb:\n",
    "                    if len_adv <= len_adj:\n",
    "                        newtext.append(adv)\n",
    "                        testing = False\n",
    "    train['corpus'].iat[i] = ' '.join(newtext)\n",
    "\n",
    "    \n",
    "#test corpus\n",
    "for i in np.arange(test.shape[0]):\n",
    "#for i in np.arange(10):\n",
    "    text = test.text.values[i] #extract text from data set train\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) #remove punctuation\n",
    "\n",
    "    #tokenize\n",
    "    text = nltk.word_tokenize(text) #tokenize the text\n",
    "\n",
    "    #stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word.lower() not in stopwords] #remove word in stopwords\n",
    "\n",
    "    #stemming & lemmatization\n",
    "    newtext = []               \n",
    "    for j, word in enumerate(text):\n",
    "        noun = lemm.lemmatize(word, wordnet.NOUN)\n",
    "        verb = lemm.lemmatize(word, wordnet.VERB)\n",
    "        adj = lemm.lemmatize(word, wordnet.ADJ)\n",
    "        adv = lemm.lemmatize(word, wordnet.ADV)\n",
    "        len_noun = len(noun)\n",
    "        len_verb = len(verb)\n",
    "        len_adj = len(adj)\n",
    "        len_adv = len(adv)\n",
    "        testing = True\n",
    "    \n",
    "        #len_noun is minimum\n",
    "        if testing:\n",
    "            if len_noun <= len_verb:\n",
    "                if len_noun <= len_adj:\n",
    "                    if len_noun <= len_adv:\n",
    "                        newtext.append(noun)\n",
    "                        testing = False\n",
    "\n",
    "        #len_verb is minimum\n",
    "        if testing:\n",
    "            if len_verb <= len_noun:\n",
    "                if len_verb <= len_adj:\n",
    "                    if len_verb <= len_adv:\n",
    "                        newtext.append(verb)\n",
    "                        testing = False\n",
    "\n",
    "        #len_adj is minimum\n",
    "        if testing:\n",
    "            if len_adj <= len_noun:\n",
    "                if len_adj <= len_verb:\n",
    "                    if len_adj <= len_adv:\n",
    "                        newtext.append(adj)\n",
    "                        testing = False\n",
    "\n",
    "        #len_adv is minimum\n",
    "        if testing:\n",
    "            if len_adv <= len_noun:\n",
    "                if len_adv <= len_verb:\n",
    "                    if len_adv <= len_adj:\n",
    "                        newtext.append(adv)\n",
    "                        testing = False\n",
    "    test['corpus'].iat[i] = ' '.join(newtext)\n",
    "    \n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "One hot encoding the author columns in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train['author'] = le.fit_transform(train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "      <td>process however afford mean ascertain dimensio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "      <td>never occur fumble might mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>left hand gold snuff box caper hill cut manner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "      <td>lovely spring look Windsor Terrace sixteen fer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "      <td>Finding nothing else even gold Superintendent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  author  \\\n",
       "0  id26305  This process, however, afforded me no means of...       0   \n",
       "1  id17569  It never once occurred to me that the fumbling...       1   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...       0   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...       2   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...       1   \n",
       "\n",
       "                                              corpus  \n",
       "0  process however afford mean ascertain dimensio...  \n",
       "1              never occur fumble might mere mistake  \n",
       "2  left hand gold snuff box caper hill cut manner...  \n",
       "3  lovely spring look Windsor Terrace sixteen fer...  \n",
       "4  Finding nothing else even gold Superintendent ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the data set now looks like ....\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Predictive Model\n",
    "We will build predictive model with:\n",
    "- tfidf \n",
    "- count features\n",
    "- logistic regression\n",
    "- naive bayes\n",
    "- grid search\n",
    "- pipeline\n",
    "First, we define the function to compute logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to compute logloss\n",
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split train data set into training and validation data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.corpus.values, train.author, \n",
    "                                                  stratify=train.author, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression with TF-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.616 \n"
     ]
    }
   ],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "\n",
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.537 \n"
     ]
    }
   ],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "\n",
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Naive Bayes on TF-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Naive Bayes on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.508 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GridSearch and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\LW130003\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.425 \n"
     ]
    }
   ],
   "source": [
    "classifier = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "classifier.fit(xtrain, ytrain)\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (0, 0.01, 0.05, 0.1, 0.3, 0.5),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters)\n",
    "gs_clf.fit(xtrain, ytrain)\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "predictions = gs_clf.predict_proba(xvalid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
